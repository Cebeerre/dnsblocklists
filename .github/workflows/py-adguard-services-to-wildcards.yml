name: Build AdGuard services -> wildcard lists (Python) + icons + SERVICES.md + README table

on:
  schedule:
    - cron: "20 3 * * 1"   # weekly: Mon 03:20 UTC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      ORIG_SOURCE: https://github.com/AdguardTeam/HostlistsRegistry/tree/main
      TREE_API: https://api.github.com/repos/AdguardTeam/HostlistsRegistry/git/trees/main?recursive=1

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests PyYAML idna

      - name: Build lists, extract icons, write SERVICES.md and update README table
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python - <<'PY'
          import os, re, sys, time, pathlib
          import requests, yaml, idna
          from requests.adapters import HTTPAdapter, Retry

          REPO = os.environ.get("GITHUB_REPOSITORY", "")
          OWNER, _, NAME = REPO.partition("/")
          PAGES_BASE = f"https://{OWNER}.github.io/{NAME}"
          TREE_API   = os.environ["TREE_API"]
          ORIG_SOURCE= os.environ["ORIG_SOURCE"]
          TOKEN      = os.environ.get("GITHUB_TOKEN", "")

          out_dir  = pathlib.Path("webservices")
          icon_dir = pathlib.Path("icons")
          out_dir.mkdir(parents=True, exist_ok=True)
          icon_dir.mkdir(parents=True, exist_ok=True)

          TS = time.strftime("%Y%m%d%H%M", time.gmtime())

          # HTTP session with retries like curl
          sess = requests.Session()
          retries = Retry(total=2, connect=2, read=2, backoff_factor=1,
                          status_forcelist=(429,500,502,503,504),
                          allowed_methods=frozenset(["GET","HEAD"]))
          adapter = HTTPAdapter(max_retries=retries, pool_connections=8, pool_maxsize=16)
          sess.mount("http://", adapter); sess.mount("https://", adapter)
          sess.headers.update({"User-Agent": "adguard-webservices-builder/2.0"})
          if TOKEN:
              sess.headers.update({"Authorization": f"Bearer {TOKEN}"})

          def api_get_json(url: str):
              r = sess.get(url, timeout=(5,20)); r.raise_for_status(); return r.json()
          def cdn_get_bytes(url: str) -> bytes:
              # raw CDN fetch (not REST rate-limited)
              r = requests.get(url, timeout=(5,20)); r.raise_for_status(); return r.content

          # --- Helpers: domain parse (simplified + IDNA) ---
          def normalize_domain_ascii(d: str) -> str|None:
              d = d.strip().strip(".").lower()
              if not d or "://" in d or "/" in d or "_" in d or " " in d or "." not in d: return None
              try: return idna.encode(d, uts46=True, std3_rules=True).decode("ascii")
              except idna.IDNAError: return None
          def parse_rule_to_domain(s: str):
              s=s.strip()
              if not s or s.startswith("#") or s.startswith("@@"): return (None,"comment")
              if s.startswith("||"):
                  m=re.match(r"^\|\|([^,^|$]+)", s)
                  if not m: return (None,"malformed_double_pipe")
                  dom=m.group(1).strip().strip(".").lower()
                  if "*" in dom: return (None,"wildcard_inside_label")
                  a=normalize_domain_ascii(dom); return (a, None if a else "invalid")
              if s.startswith("|"): return (None,"single_pipe")
              if s.startswith("*."):
                  rest=s[2:].strip().strip(".").lower()
                  if "*" in rest: return (None,"wildcard_inside_label")
                  a=normalize_domain_ascii(rest); return (a, None if a else "invalid")
              plain=re.split(r"[,|$]", s, 1)[0].strip().strip(".").lower()
              if "*" in plain: return (None,"wildcard_inside_label")
              a=normalize_domain_ascii(plain); return (a, None if a else "invalid")

          # 1) Discover service YAMLs via a single recursive tree call
          tree = api_get_json(TREE_API)
          files = [t["path"] for t in tree.get("tree", []) if t.get("type")=="blob"]
          svc_paths = [p for p in files if p.startswith("services/") and (p.endswith(".yml") or p.endswith(".yaml"))]
          print(f"Service YAML files found: {len(svc_paths)}")

          upstream_sids=set(); icons_written=0

          # 2) Process each YAML
          for rel in sorted(svc_paths):
              raw_url=f"https://raw.githubusercontent.com/AdguardTeam/HostlistsRegistry/main/{rel}"
              try:
                  yml=yaml.safe_load(cdn_get_bytes(raw_url)) or {}
              except Exception as e:
                  print(f"WARN: YAML parse failed for {rel}: {e}", file=sys.stderr); continue

              sid_raw=(yml.get("id") or yml.get("name") or "unknown")
              sname  =(yml.get("name") or yml.get("id") or "unknown")
              sid=re.sub(r"[^a-z0-9]+","_", str(sid_raw).lower()).strip("_") or "unknown"
              upstream_sids.add(sid)

              # icon_svg â†’ icons/<sid>.svg
              icon_svg=yml.get("icon_svg")
              if isinstance(icon_svg,str) and "<svg" in icon_svg.lower():
                  (icon_dir/f"{sid}.svg").write_text(icon_svg.strip(), encoding="utf-8")
                  icons_written+=1

              # rules â†’ domains
              rules=yml.get("rules") or []
              if not isinstance(rules,list): rules=[]
              domains=set(); skipped=[]
              for line in rules:
                  if not isinstance(line,str): continue
                  a,reason=parse_rule_to_domain(line)
                  if a: domains.add(a)
                  elif reason not in ("comment",): skipped.append(line)

              entries_count = len(domains)
              out_path=out_dir/f"{sid}_asterisk.txt"
              with out_path.open("w", encoding="utf-8") as f:
                  f.write(f"# Title: Blocklist for {sname}\n")
                  f.write(f"# Description: Blocks {sname} content (sourced from AdGuardTeam's HostlistsRegistry)\n")
                  f.write(f"# Homepage: https://github.com/{OWNER}/{NAME}\n")
                  f.write(f"# License: https://github.com/{OWNER}/{NAME}/blob/main/LICENSE\n")
                  f.write(f"# Version: {TS}\n")
                  f.write(f"# Original Source: {ORIG_SOURCE}\n")
                  f.write(f"# Syntax: Domains Wildcard - Blocky (v0.23 or newer)\n")
                  f.write(f"# Entries: {entries_count}\n#\n")
                  if skipped:
                      f.write("# Skipped unsupported rules:\n")
                      for s in skipped: f.write(f"# Skipped unsupported rule: {s}\n")
                      f.write("#\n")
                  for d in sorted(domains):
                      f.write(f"*.{d}\n")

          print(f"Icons written: {icons_written}")

          # 3) Sync deletions for lists/icons no longer upstream
          def sid_from_txt(p: pathlib.Path)->str: return p.name.removesuffix("_asterisk.txt")
          for p in list(out_dir.glob("*_asterisk.txt")):
              if sid_from_txt(p) not in upstream_sids:
                  p.unlink(missing_ok=True)
          for p in list(icon_dir.glob("*.svg")):
              if p.stem not in upstream_sids:
                  p.unlink(missing_ok=True)

          # 4) Build rows for both SERVICES.md and README injection (20x20 icons)
          rows=[]
          for p in sorted(out_dir.glob("*_asterisk.txt")):
              sid=p.name.removesuffix("_asterisk.txt")
              name_pretty=re.sub(r"_+"," ", sid).title()
              pages_url=f"{PAGES_BASE}/webservices/{p.name}"
              icon_rel=f"icons/{sid}.svg"
              icon_md = f'<img src="{icon_rel}" width="20" height="20"/>' if pathlib.Path(icon_rel).exists() else "â€”"
              link_md=f"[{p.name}]({pages_url})"
              rows.append((icon_md, name_pretty, link_md))

          # 5) Write SERVICES.md (full doc)
          md = []
          md.append("## ðŸ“‹ Webservices Blocklists\n")
          md.append("Automatically generated from AdGuard's HostlistsRegistry. Each list is already in wildcard format for Blocky.\n\n")
          md.append("| Icon | Name | Link |\n|------|------|------|\n")
          for icon,name,link in rows:
              md.append(f"| {icon} | {name} | {link} |\n")
          pathlib.Path("SERVICES.md").write_text("".join(md), encoding="utf-8")

          # 6) Inject FULL TABLE (header + rows) into README between markers
          readme_path=pathlib.Path("README.md")
          readme_text=readme_path.read_text(encoding="utf-8")

          table_only = []
          table_only.append("| Icon | Service | Link |\n")
          table_only.append("|------|---------|------|\n")
          for icon,name,link in rows:
              table_only.append(f"| {icon} | {name} | {link} |\n")
          table_only = "".join(table_only)

          start="<!-- START:SERVICES -->"
          end="<!-- END:SERVICES -->"
          new_text = re.sub(
              rf"{re.escape(start)}.*?{re.escape(end)}",
              f"{start}\n{table_only}{end}",
              readme_text,
              flags=re.DOTALL
          )
          if new_text != readme_text:
              readme_path.write_text(new_text, encoding="utf-8")
              print(f"README: services table injected ({len(rows)} rows)")
          else:
              print("README already up to date")
          PY

      - name: Commit & push if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(services): refresh lists, icons, SERVICES.md, inject README table (20x20 icons)"
          add_options: "-A"
