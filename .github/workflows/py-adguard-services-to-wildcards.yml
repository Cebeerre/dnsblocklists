name: Build AdGuard services -> wildcard lists (Python) + icons + README table (sync)

on:
  schedule:
    - cron: "20 3 * * 1"   # weekly: Mon 03:20 UTC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      SRC_DIR_ENDPOINT: https://api.github.com/repos/AdguardTeam/HostlistsRegistry/contents/services
      ORIG_SOURCE: https://github.com/AdguardTeam/HostlistsRegistry/tree/main

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests PyYAML idna

      - name: Build lists, extract icons, sync deletions, update README table
        run: |
          python - <<'PY'
          import os, re, sys, time, json, pathlib, textwrap
          import requests, yaml, idna
          from requests.adapters import HTTPAdapter, Retry

          REPO = os.environ.get("GITHUB_REPOSITORY", "")
          OWNER, _, NAME = REPO.partition("/")
          PAGES_BASE = f"https://{OWNER}.github.io/{NAME}"
          SRC_DIR_ENDPOINT = os.environ["SRC_DIR_ENDPOINT"]
          ORIG_SOURCE  = os.environ["ORIG_SOURCE"]

          out_dir  = pathlib.Path("webservices")
          icon_dir = pathlib.Path("icons")
          out_dir.mkdir(parents=True, exist_ok=True)
          icon_dir.mkdir(parents=True, exist_ok=True)

          TS = time.strftime("%Y%m%d%H%M", time.gmtime())

          # HTTP session (retries + timeouts similar to your curl flags)
          sess = requests.Session()
          retries = Retry(
              total=2, connect=2, read=2, backoff_factor=1,
              status_forcelist=(429, 500, 502, 503, 504),
              allowed_methods=frozenset(["GET", "HEAD"])
          )
          adapter = HTTPAdapter(max_retries=retries, pool_connections=4, pool_maxsize=8)
          sess.mount("http://", adapter)
          sess.mount("https://", adapter)
          sess.headers.update({"User-Agent": "adguard-webservices-builder/1.4"})

          def http_get_json(url: str):
              r = sess.get(url, timeout=(5, 20))
              r.raise_for_status()
              return r.json()

          def http_get_bytes(url: str) -> bytes:
              r = sess.get(url, timeout=(5, 20))
              r.raise_for_status()
              return r.content

          # RFC-ish + IDNA domain validator
          _label_re = re.compile(r"^[a-z0-9-]{1,63}$", re.IGNORECASE)
          def _is_valid_label(label: str) -> bool:
              return bool(label) and len(label) <= 63 and label[0] != '-' and label[-1] != '-' and _label_re.fullmatch(label)

          def normalize_domain_ascii(domain: str) -> str | None:
              d = domain.strip().strip(".").lower()
              if not d or "://" in d or "/" in d or "_" in d or " " in d or "." not in d:
                  return None
              try:
                  ascii_d = idna.encode(d, uts46=True, std3_rules=True).decode("ascii")
              except idna.IDNAError:
                  return None
              if len(ascii_d) > 253:
                  return None
              labels = ascii_d.split(".")
              if len(labels) < 2 or not all(_is_valid_label(l) for l in labels):
                  return None
              tld = labels[-1]
              if not (re.fullmatch(r"[a-z]{2,63}", tld) or re.fullmatch(r"xn--[a-z0-9-]{1,59}", tld)):
                  return None
              return ascii_d

          def parse_rule_to_domain(rule: str) -> tuple[str | None, str | None]:
              s = rule.strip()
              if not s or s.startswith("#") or s.startswith("@@"):
                  return (None, "comment_or_exception")
              if s.startswith("||"):
                  m = re.match(r"^\|\|([^,^|$]+)", s)
                  if not m: return (None, "malformed_double_pipe")
                  dom = m.group(1).strip().strip(".").lower()
                  if "*" in dom: return (None, "wildcard_inside_label")
                  dom_ascii = normalize_domain_ascii(dom)
                  return (dom_ascii, None if dom_ascii else "invalid_domain")
              if s.startswith("|"):
                  return (None, "single_pipe_anchor")
              if s.startswith("*."):
                  rest = s[2:].strip().strip(".").lower()
                  if "*" in rest: return (None, "wildcard_inside_label")
                  dom_ascii = normalize_domain_ascii(rest)
                  return (dom_ascii, None if dom_ascii else "invalid_domain")
              plain = re.split(r"[,|$]", s, maxsplit=1)[0].strip().strip(".").lower()
              if "*" in plain: return (None, "wildcard_inside_label")
              dom_ascii = normalize_domain_ascii(plain)
              return (dom_ascii, None if dom_ascii else "invalid_domain")

          # 1) List ALL YAMLs in services/ with pagination
          print("Fetching services directory listing with pagination…")
          page = 1
          per_page = 100
          entries = []
          while True:
              url = f"{SRC_DIR_ENDPOINT}?ref=main&per_page={per_page}&page={page}"
              chunk = http_get_json(url)
              if not isinstance(chunk, list) or not chunk:
                  break
              entries.extend(chunk)
              page += 1
          print(f"Directory entries fetched: {len(entries)}")

          upstream_sids: set[str] = set()
          services_count = 0
          icons_written = 0

          # 2) Process each service YAML
          for entry in entries:
              if entry.get("type") != "file":
                  continue
              raw_url = entry.get("download_url")
              if not raw_url:
                  continue

              name = entry.get("name", "unknown")
              try:
                  yml = yaml.safe_load(http_get_bytes(raw_url)) or {}
              except Exception as e:
                  print(f"WARN: YAML parse failed for {name}: {e}", file=sys.stderr)
                  continue

              sid_raw = (yml.get("id") or yml.get("name") or "unknown")
              sname   = (yml.get("name") or yml.get("id") or "unknown")
              sid = re.sub(r"[^a-z0-9]+", "_", str(sid_raw).lower()).strip("_") or "unknown"
              upstream_sids.add(sid)
              services_count += 1

              # Save icon from per-service YAML
              icon_svg = yml.get("icon_svg")
              if isinstance(icon_svg, str) and "<svg" in icon_svg.lower():
                  try:
                      (icon_dir / f"{sid}.svg").write_text(icon_svg.strip(), encoding="utf-8")
                      icons_written += 1
                  except Exception as e:
                      print(f"WARN: could not write icon for {sid}: {e}", file=sys.stderr)

              # Build domains
              rules = yml.get("rules") or []
              if not isinstance(rules, list): rules = []
              domains = set()
              skipped = []

              for line in rules:
                  if not isinstance(line, str): continue
                  dom_ascii, reason = parse_rule_to_domain(line)
                  if dom_ascii:
                      domains.add(dom_ascii)
                  else:
                      if reason not in ("comment_or_exception",):
                          skipped.append(f"# Skipped unsupported rule: {line}")

              sorted_wild = [f"*.{d}" for d in sorted(domains)]
              out_path = out_dir / f"{sid}_asterisk.txt"
              with out_path.open("w", encoding="utf-8") as f:
                  f.write(f"# Title: Blocklist for {sname}\n")
                  f.write(f"# Description: Blocks {sname} content (sourced from AdGuardTeam's HostlistsRegistry)\n")
                  f.write(f"# Homepage: https://github.com/{OWNER}/{NAME}\n")
                  f.write(f"# License: https://github.com/{OWNER}/{NAME}/blob/main/LICENSE\n")
                  f.write(f"# Version: {TS}\n")
                  f.write(f"# Original Source: {ORIG_SOURCE}\n")
                  f.write(f"# Syntax: Domains Wildcard - Blocky (v0.23 or newer)\n")
                  f.write("#\n")
                  if skipped:
                      f.write("# Skipped unsupported rules:\n")
                      for s in skipped: f.write(s + "\n")
                      f.write("#\n")
                  for w in sorted_wild: f.write(w + "\n")

          print(f"Services processed: {services_count}, icons written: {icons_written}")

          # 3) Sync deletions: lists & icons no longer in upstream
          def sid_from_txt(path: pathlib.Path) -> str:
              return path.name.removesuffix("_asterisk.txt")

          for p in list(out_dir.glob("*_asterisk.txt")):
              if sid_from_txt(p) not in upstream_sids:
                  print(f"Removing obsolete list: {p}")
                  try: p.unlink()
                  except FileNotFoundError: pass

          for p in list(icon_dir.glob("*.svg")):
              if p.stem not in upstream_sids:
                  print(f"Removing obsolete icon: {p}")
                  try: p.unlink()
                  except FileNotFoundError: pass

          # 4) Update README table (Icon | Name | Link)
          readme_path = pathlib.Path("README.md")
          if not readme_path.exists():
              print("README.md not found, skipping table update.")
              sys.exit(0)

          rows = []
          for p in sorted(out_dir.glob("*_asterisk.txt")):
              sid = p.name.removesuffix("_asterisk.txt")
              name_pretty = re.sub(r"_+", " ", sid).title()
              pages_url = f"{PAGES_BASE}/webservices/{p.name}"
              icon_rel = f"icons/{sid}.svg"
              icon_md = f"![]({icon_rel})" if pathlib.Path(icon_rel).exists() else "—"
              link_md = f"[{p.name}]({pages_url})"
              rows.append((icon_md, name_pretty, link_md))

          header = textwrap.dedent("""\
          ## 📋 Webservices Blocklists

          Automatically generated from AdGuard's HostlistsRegistry. Each list is already in wildcard format for Blocky.

          | Icon | Name | Link |
          |------|------|------|
          """)
          table_md = header + "".join(f"| {icon} | {name} | {link} |\n" for icon, name, link in rows)

          START = r"<!--\s*BEGIN:SERVICES_TABLE\s*-->"
          END   = r"<!--\s*END:SERVICES_TABLE\s*-->"
          readme = readme_path.read_text(encoding="utf-8")
          pattern = re.compile(f"{START}.*?{END}", re.DOTALL | re.IGNORECASE)
          new_block = f"<!-- BEGIN:SERVICES_TABLE -->\n<!-- This section is auto-generated. Do not edit by hand. -->\n\n{table_md}\n<!-- END:SERVICES_TABLE -->"

          if re.search(pattern, readme):
              new_readme = re.sub(pattern, new_block, readme)
          else:
              new_readme = f"{readme.rstrip()}\n\n{new_block}\n"

          if new_readme != readme:
              readme_path.write_text(new_readme, encoding="utf-8")
              print(f"README updated. Rows: {len(rows)}")
          else:
              print(f"README up-to-date. Rows: {len(rows)}")
          PY

      - name: Commit & push if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(services): refresh lists, icons, README table (sync with upstream)"
          file_pattern: |
            webservices/*.txt
            icons/*.svg
            README.md
